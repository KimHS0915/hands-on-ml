{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indian-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interstate-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "congressional-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-position",
   "metadata": {},
   "source": [
    "Exercise 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "equipped-deposit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['January',\n",
       " 'February',\n",
       " 'March',\n",
       " 'April',\n",
       " 'May',\n",
       " 'June',\n",
       " 'July',\n",
       " 'August',\n",
       " 'September',\n",
       " 'October',\n",
       " 'November',\n",
       " 'December']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import calendar\n",
    "\n",
    "MONTHS = calendar.month_name[1:]\n",
    "MONTHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "perfect-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "    \n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "    \n",
    "    x = [MONTHS[dt.month - 1] + ' ' + dt.strftime('%d, %Y') for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "determined-puppy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "September 20, 7075       7075-09-20               \n",
      "May 15, 8579             8579-05-15               \n",
      "January 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
    "print(\"-\" * 50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "subject-machine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ,0123456789ADFJMNOSabceghilmnoprstuvy'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS = ''.join(sorted(set(''.join(MONTHS) + '0123456789, ')))\n",
    "INPUT_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "based-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHARS = '0123456789-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pediatric-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "third-absence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "September 20, 7075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19, 23, 31, 34, 23, 28, 21, 23, 32, 0, 4, 2, 1, 0, 9, 2, 9, 7]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_example[0])\n",
    "date_str_to_ids(x_example[0], INPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "supposed-circle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7075-09-20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_example[0])\n",
    "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "irish-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    x_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    x = tf.ragged.constant(x_ids, ragged_rank=1)\n",
    "    return (x + 1).to_tensor()\n",
    "\n",
    "def create_dateset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "flush-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x_train, y_train = create_dateset(10000)\n",
    "x_valid, y_valid = create_dateset(2000)\n",
    "x_test, y_test = create_dateset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "married-yesterday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(18,), dtype=int32, numpy=\n",
       "array([20, 24, 32, 35, 24, 29, 22, 24, 33,  1,  5,  3,  2,  1, 10,  3, 10,\n",
       "        8])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "medium-ireland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-yemen",
   "metadata": {},
   "source": [
    "basic seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecological-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = y_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1, \n",
    "                           output_dim=embedding_size, \n",
    "                           input_shape=[None]),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation='softmax')\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(max_output_length),\n",
    "    decoder\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "proved-momentum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 7s 15ms/step - loss: 1.8040 - acc: 0.3534 - val_loss: 1.3620 - val_acc: 0.4954\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 1.2083 - acc: 0.5599 - val_loss: 1.0384 - val_acc: 0.6218\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.2340 - acc: 0.5710 - val_loss: 1.0927 - val_acc: 0.6130\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.8616 - acc: 0.6829 - val_loss: 0.7503 - val_acc: 0.7151\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6409 - acc: 0.7538 - val_loss: 0.6277 - val_acc: 0.7466\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.4652 - acc: 0.8169 - val_loss: 0.3821 - val_acc: 0.8492\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.5064 - acc: 0.8149 - val_loss: 0.3457 - val_acc: 0.8672\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2698 - acc: 0.9013 - val_loss: 0.2164 - val_acc: 0.9257\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.2261 - acc: 0.9366 - val_loss: 0.1368 - val_acc: 0.9637\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0954 - acc: 0.9788 - val_loss: 0.0717 - val_acc: 0.9862\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1977 - acc: 0.9520 - val_loss: 0.1750 - val_acc: 0.9573\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0651 - acc: 0.9899 - val_loss: 0.0398 - val_acc: 0.9951\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0272 - acc: 0.9976 - val_loss: 0.0229 - val_acc: 0.9980\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0166 - acc: 0.9991 - val_loss: 0.0158 - val_acc: 0.9990\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.0290 - acc: 0.9958 - val_loss: 0.0137 - val_acc: 0.9991\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.0087 - acc: 0.9998 - val_loss: 0.0081 - val_acc: 0.9997\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0057 - acc: 0.9999 - val_loss: 0.0058 - val_acc: 0.9998\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 0.9999\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='nadam', \n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20, \n",
    "                    validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "prerequisite-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return [''.join([('?' + chars)[index] for index in sequence]) \n",
    "            for sequence in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "positive-dependence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "x_new = prepare_date_strs(['September 17, 2009', 'July 14, 1789'])\n",
    "ids = np.argmax(model.predict(x_new), axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bright-saudi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-02\n",
      "1789-01-14\n"
     ]
    }
   ],
   "source": [
    "x_new = prepare_date_strs(['May 02, 2020', 'July 14, 1789'])\n",
    "ids = np.argmax(model.predict(x_new), axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "particular-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = x_train.shape[1]\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    x = prepare_date_strs(date_strs)\n",
    "    if x.shape[1] < max_input_length:\n",
    "        x = tf.pad(x, [[0, 0], [0, max_input_length - x.shape[1]]])\n",
    "    return x\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    x = prepare_date_strs_padded(date_strs)\n",
    "    ids = np.argmax(model.predict(x), axis=-1)\n",
    "    return ids_to_date_strs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hairy-producer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_strs(['May 02, 2020', 'July 14, 1789'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-stewart",
   "metadata": {},
   "source": [
    "feeding the shifted targets to the decoder (teacher forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "annual-flavor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "sos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "double-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_output_sequences(y):\n",
    "    sos_tokens = tf.fill(dims=(len(y), 1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, y[:, :-1]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ignored-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_decoder = shifted_output_sequences(y_train)\n",
    "x_valid_decoder = shifted_output_sequences(y_valid)\n",
    "x_test_decoder = shifted_output_sequences(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ranking-philosophy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
       "array([[12,  8,  1, ..., 10, 11,  3],\n",
       "       [12,  9,  6, ...,  6, 11,  2],\n",
       "       [12,  8,  2, ...,  2, 11,  2],\n",
       "       ...,\n",
       "       [12, 10,  8, ...,  2, 11,  4],\n",
       "       [12,  2,  2, ...,  3, 11,  3],\n",
       "       [12,  8,  9, ...,  8, 11,  3]])>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "municipal-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(INPUT_CHARS) + 1, \n",
    "    output_dim=encoder_embedding_size)(encoder_input)\n",
    "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(\n",
    "    lstm_units, return_state=True)(encoder_embedding)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(OUTPUT_CHARS) + 2, \n",
    "    output_dim=decoder_embedding_size)(decoder_input)\n",
    "decoder_lstm_output = keras.layers.LSTM(\n",
    "    lstm_units, return_sequences=True)(decoder_embedding, \n",
    "                                       initial_state=encoder_state)\n",
    "decoder_output = keras.layers.Dense(\n",
    "    len(OUTPUT_CHARS) + 1, activation='softmax')(decoder_lstm_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input, decoder_input], \n",
    "                           outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "promotional-kitty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 7s 15ms/step - loss: 1.6804 - acc: 0.3741 - val_loss: 1.4205 - val_acc: 0.4508\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 1.1836 - acc: 0.5606 - val_loss: 0.8859 - val_acc: 0.6757\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6046 - acc: 0.7867 - val_loss: 0.3560 - val_acc: 0.8884\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1872 - acc: 0.9571 - val_loss: 0.1117 - val_acc: 0.9790\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0821 - acc: 0.9880 - val_loss: 0.0399 - val_acc: 0.9981\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.0374 - acc: 0.9964 - val_loss: 0.0269 - val_acc: 0.9985\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0159 - acc: 0.9998 - val_loss: 0.0129 - val_acc: 0.9998\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0552 - acc: 0.9897 - val_loss: 0.0133 - val_acc: 0.9998\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='nadam', \n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit([x_train, x_train_decoder], y_train, epochs=10, \n",
    "                    validation_data=([x_valid, x_valid_decoder], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "checked-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def predict_date_strs(date_strs):\n",
    "    x = prepare_date_strs_padded(date_strs)\n",
    "    y_pred = tf.fill(dims=(len(x), 1), value=sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - y_pred.shape[1]\n",
    "        x_decoder = tf.pad(y_pred, [[0, 0], [0, pad_size]])\n",
    "        y_probas_next = model.predict([x, x_decoder])[:, index:index+1]\n",
    "        y_pred_next = tf.argmax(y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        y_pred = tf.concat([y_pred, y_pred_next], axis=1)\n",
    "    return ids_to_date_strs(y_pred[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "polish-confidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs(['May 02, 2020', 'July 14, 1789'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-cornell",
   "metadata": {},
   "source": [
    "TF-Addons seq2seq implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "stopped-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "subtle-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    len(OUTPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, \n",
    "                                                 sampler, \n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "y_proba = keras.layers.Activation('softmax')(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "earlier-array",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 22s 64ms/step - loss: 1.6805 - acc: 0.3740 - val_loss: 1.4190 - val_acc: 0.4527\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 19s 61ms/step - loss: 1.1887 - acc: 0.5589 - val_loss: 0.8898 - val_acc: 0.6733\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 19s 60ms/step - loss: 0.6265 - acc: 0.7774 - val_loss: 0.3592 - val_acc: 0.8906\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 19s 61ms/step - loss: 0.2069 - acc: 0.9513 - val_loss: 0.1108 - val_acc: 0.9815\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 19s 61ms/step - loss: 0.0747 - acc: 0.9906 - val_loss: 0.0431 - val_acc: 0.9979\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 19s 62ms/step - loss: 0.0688 - acc: 0.9891 - val_loss: 0.0304 - val_acc: 0.9984\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 0.0186 - acc: 0.9997 - val_loss: 0.0145 - val_acc: 0.9999\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 20s 64ms/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.0097 - val_acc: 0.9999\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 20s 63ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 0.9999\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 20s 63ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 20s 63ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 20s 63ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 19s 61ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 20s 64ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 20s 64ms/step - loss: 0.0748 - acc: 0.9837 - val_loss: 0.3979 - val_acc: 0.8738\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='nadam', \n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit([x_train, x_train_decoder], y_train, epochs=15, \n",
    "                    validation_data=([x_valid, x_valid_decoder], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "operating-science",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-03-02', '1789-07-14']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs(['May 02, 2020', 'July 14, 1789'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "manufactured-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "    embedding_fn=decoder_embedding_layer)\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    decoder_cell, inference_sampler, output_layer=output_layer, \n",
    "    maximum_iterations=max_output_length)\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens, initial_state=encoder_state, \n",
    "    start_tokens=start_tokens, end_token=0)\n",
    "\n",
    "inference_model = keras.models.Model(inputs=[encoder_inputs], \n",
    "                                     outputs=[final_outputs.sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dimensional-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_predict_date_strs(date_strs):\n",
    "    x = prepare_date_strs_padded(date_strs)\n",
    "    y_pred = inference_model.predict(x)\n",
    "    return ids_to_date_strs(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "banner-deposit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-03-02', '1789-07-14']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_predict_date_strs(['May 02, 2020', 'July 14, 1789'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "animal-passage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 ms ± 13.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit predict_date_strs(['May 02, 2020', 'July 14, 1789'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "double-sterling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.6 ms ± 852 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fast_predict_date_strs((['May 02, 2020', 'July 14, 1789']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-narrative",
   "metadata": {},
   "source": [
    "TF-Addons seq2seq implementation with scheduled sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "sound-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "pressed-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    len(OUTPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.ScheduledEmbeddingTrainingSampler(\n",
    "    sampling_probability=0., embedding_fn=decoder_embedding_layer)\n",
    "sampler.sampling_probability = tf.Variable(0.)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, \n",
    "                                                 sampler, \n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "y_proba = keras.layers.Activation('softmax')(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "steady-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sampling_probability(epoch, logs):\n",
    "    proba = min(1.0, epoch / (n_epochs - 10))\n",
    "    sampler.sampling_probability.assign(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dated-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_probability_cb = keras.callbacks.LambdaCallback(\n",
    "    on_epoch_begin=update_sampling_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "residential-discussion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 31s 92ms/step - loss: 1.6805 - acc: 0.3740 - val_loss: 1.4187 - val_acc: 0.4535\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 29s 93ms/step - loss: 1.2097 - acc: 0.5534 - val_loss: 0.9472 - val_acc: 0.6436\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 28s 90ms/step - loss: 0.7269 - acc: 0.7320 - val_loss: 0.5345 - val_acc: 0.8063\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 28s 90ms/step - loss: 0.4015 - acc: 0.8681 - val_loss: 0.2988 - val_acc: 0.9096\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 28s 91ms/step - loss: 0.2279 - acc: 0.9374 - val_loss: 0.1736 - val_acc: 0.9586\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 29s 94ms/step - loss: 0.1338 - acc: 0.9689 - val_loss: 0.0950 - val_acc: 0.9801\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 0.1413 - acc: 0.9686 - val_loss: 0.0723 - val_acc: 0.9850\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 0.0515 - acc: 0.9902 - val_loss: 0.0413 - val_acc: 0.9919\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 28s 91ms/step - loss: 0.0298 - acc: 0.9951 - val_loss: 0.0246 - val_acc: 0.9956\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 28s 91ms/step - loss: 0.0194 - acc: 0.9969 - val_loss: 0.0170 - val_acc: 0.9976\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 28s 89ms/step - loss: 0.0131 - acc: 0.9981 - val_loss: 0.0132 - val_acc: 0.9977\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 29s 91ms/step - loss: 0.0081 - acc: 0.9991 - val_loss: 0.0067 - val_acc: 0.9990\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 29s 93ms/step - loss: 0.0061 - acc: 0.9993 - val_loss: 0.0055 - val_acc: 0.9994\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 28s 89ms/step - loss: 0.0045 - acc: 0.9995 - val_loss: 0.0037 - val_acc: 0.9997\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 29s 92ms/step - loss: 0.0029 - acc: 0.9998 - val_loss: 0.0024 - val_acc: 0.9998\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.0023 - acc: 0.9998 - val_loss: 0.0029 - val_acc: 0.9997\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 30s 96ms/step - loss: 0.1486 - acc: 0.9579 - val_loss: 0.0092 - val_acc: 0.9992\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 30s 95ms/step - loss: 0.0053 - acc: 0.9997 - val_loss: 0.0038 - val_acc: 0.9998\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 30s 96ms/step - loss: 0.0029 - acc: 0.9998 - val_loss: 0.0027 - val_acc: 0.9998\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 30s 96ms/step - loss: 0.0021 - acc: 0.9998 - val_loss: 0.0023 - val_acc: 0.9998\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='nadam', \n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit([x_train, x_train_decoder], y_train, epochs=n_epochs, \n",
    "                    validation_data=([x_valid, x_valid_decoder], y_valid), \n",
    "                    callbacks=[sampling_probability_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "funky-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_temperature = tf.Variable(1.)\n",
    "\n",
    "inference_sampler = tfa.seq2seq.sampler.SampleEmbeddingSampler(\n",
    "    embedding_fn=decoder_embedding_layer, \n",
    "    softmax_temperature=softmax_temperature)\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    decoder_cell, inference_sampler, output_layer=output_layer, \n",
    "    maximum_iterations=max_output_length)\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens, initial_state=encoder_state, \n",
    "    start_tokens=start_tokens, end_token=0)\n",
    "\n",
    "inference_model = keras.models.Model(inputs=[encoder_inputs], \n",
    "                                     outputs=[final_outputs.sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "black-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creative_predict_date_strs(date_strs, temperature=1.0):\n",
    "    softmax_temperature.assign(temperature)\n",
    "    x = prepare_date_strs_padded(date_strs)\n",
    "    y_pred = inference_model.predict(x)\n",
    "    return ids_to_date_strs(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "parallel-study",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creative_predict_date_strs(['May 02, 2020', 'July 14, 1789'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cosmetic-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9122--3000', '1789-06411']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creative_predict_date_strs(['May 02, 2020', 'July 14, 1789'], \n",
    "                           temperature=5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-record",
   "metadata": {},
   "source": [
    "TFA seq2seq, the Keras subclassing API and attention mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "linear-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTranslation(keras.models.Model):\n",
    "    def __init__(self, units=128, encoder_embedding_size=32, \n",
    "                 decoder_embedding_size=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder_embedding = keras.layers.Embedding(\n",
    "            input_dim=len(INPUT_CHARS) + 1, \n",
    "            output_dim=encoder_embedding_size)\n",
    "        self.encoder = keras.layers.LSTM(units, \n",
    "                                         return_sequences=True, \n",
    "                                         return_state=True)\n",
    "        self.decoder_embedding = keras.layers.Embedding(\n",
    "            input_dim=len(OUTPUT_CHARS) + 2, \n",
    "            output_dim=decoder_embedding_size)\n",
    "        self.attention = tfa.seq2seq.LuongAttention(units)\n",
    "        decoder_inner_cell = keras.layers.LSTMCell(units)\n",
    "        self.decoder_cell = tfa.seq2seq.AttentionWrapper(\n",
    "            cell=decoder_inner_cell, \n",
    "            attention_mechanism=self.attention)\n",
    "        output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell, \n",
    "            sampler=tfa.seq2seq.sampler.TrainingSampler(), \n",
    "            output_layer=output_layer)\n",
    "        self.inference_decoder = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell, \n",
    "            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "                embedding_fn=self.decoder_embedding), \n",
    "            output_layer=output_layer, \n",
    "            maximum_iterations=max_output_length)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        encoder_input, decoder_input = inputs\n",
    "        encoder_embeddings = self.encoder_embedding(encoder_input)\n",
    "        encoder_outputs, encoder_state_h, encoder_state_c = self.encoder(\n",
    "            encoder_embeddings, \n",
    "            training=training)\n",
    "        encoder_state = [encoder_state_h, encoder_state_c]\n",
    "        \n",
    "        self.attention(encoder_outputs, setup_memory=True)\n",
    "        \n",
    "        decoder_embeddings = self.decoder_embedding(decoder_input)\n",
    "        decoder_initial_state = self.decoder_cell.get_initial_state(\n",
    "            decoder_embeddings)\n",
    "        decoder_initial_state = decoder_initial_state.clone(\n",
    "            cell_state=encoder_state)\n",
    "        \n",
    "        if training:\n",
    "            decoder_outputs, _, _ = self.decoder(\n",
    "                decoder_embeddings, \n",
    "                initial_state=decoder_initial_state, \n",
    "                training=training)\n",
    "            \n",
    "        else:\n",
    "            start_tokens = tf.zeros_like(encoder_input[:, 0]) + sos_id\n",
    "            decoder_outputs, _, _ = self.inference_decoder(\n",
    "                decoder_embeddings, \n",
    "                initial_state=decoder_initial_state, \n",
    "                start_tokens=start_tokens, \n",
    "                end_token=0)\n",
    "        \n",
    "        return tf.nn.softmax(decoder_outputs.rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "focal-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "every-tunnel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "313/313 [==============================] - 34s 94ms/step - loss: 2.1495 - acc: 0.2295 - val_loss: 2.0183 - val_acc: 0.2701\n",
      "Epoch 2/25\n",
      "313/313 [==============================] - 28s 89ms/step - loss: 1.7755 - acc: 0.3550 - val_loss: 1.4173 - val_acc: 0.4688\n",
      "Epoch 3/25\n",
      "313/313 [==============================] - 28s 91ms/step - loss: 1.3169 - acc: 0.5069 - val_loss: 1.2829 - val_acc: 0.5161\n",
      "Epoch 4/25\n",
      "313/313 [==============================] - 27s 87ms/step - loss: 2.0855 - acc: 0.2573 - val_loss: 2.7044 - val_acc: 0.1744\n",
      "Epoch 5/25\n",
      "313/313 [==============================] - 28s 88ms/step - loss: 1.6903 - acc: 0.4149 - val_loss: 1.4559 - val_acc: 0.4953\n",
      "Epoch 6/25\n",
      "313/313 [==============================] - 28s 89ms/step - loss: 1.3538 - acc: 0.5172 - val_loss: 1.2555 - val_acc: 0.5397\n",
      "Epoch 7/25\n",
      "313/313 [==============================] - 27s 87ms/step - loss: 1.1829 - acc: 0.5595 - val_loss: 1.1563 - val_acc: 0.5650\n",
      "Epoch 8/25\n",
      "313/313 [==============================] - 27s 87ms/step - loss: 1.0142 - acc: 0.5940 - val_loss: 1.0758 - val_acc: 0.5938\n",
      "Epoch 9/25\n",
      "313/313 [==============================] - 28s 89ms/step - loss: 0.8372 - acc: 0.6395 - val_loss: 1.0347 - val_acc: 0.6203\n",
      "Epoch 10/25\n",
      "313/313 [==============================] - 30s 95ms/step - loss: 0.7283 - acc: 0.6808 - val_loss: 0.8826 - val_acc: 0.6733\n",
      "Epoch 11/25\n",
      "313/313 [==============================] - 28s 90ms/step - loss: 0.6240 - acc: 0.7318 - val_loss: 0.9040 - val_acc: 0.6847\n",
      "Epoch 12/25\n",
      "313/313 [==============================] - 28s 90ms/step - loss: 0.5411 - acc: 0.7722 - val_loss: 0.7277 - val_acc: 0.7380\n",
      "Epoch 13/25\n",
      "313/313 [==============================] - 28s 90ms/step - loss: 0.4541 - acc: 0.8153 - val_loss: 0.5783 - val_acc: 0.7979\n",
      "Epoch 14/25\n",
      "313/313 [==============================] - 28s 91ms/step - loss: 0.3738 - acc: 0.8576 - val_loss: 0.4080 - val_acc: 0.8585\n",
      "Epoch 15/25\n",
      "313/313 [==============================] - 28s 90ms/step - loss: 0.3141 - acc: 0.8901 - val_loss: 0.3475 - val_acc: 0.8603\n",
      "Epoch 16/25\n",
      "313/313 [==============================] - 28s 90ms/step - loss: 0.2438 - acc: 0.9268 - val_loss: 0.2623 - val_acc: 0.9069\n",
      "Epoch 17/25\n",
      "313/313 [==============================] - 28s 90ms/step - loss: 0.2023 - acc: 0.9527 - val_loss: 0.2124 - val_acc: 0.9513\n",
      "Epoch 18/25\n",
      "313/313 [==============================] - 27s 86ms/step - loss: 0.1694 - acc: 0.9661 - val_loss: 0.1645 - val_acc: 0.9704\n",
      "Epoch 19/25\n",
      "313/313 [==============================] - 27s 86ms/step - loss: 0.1291 - acc: 0.9816 - val_loss: 0.1312 - val_acc: 0.9807\n",
      "Epoch 20/25\n",
      "313/313 [==============================] - 27s 87ms/step - loss: 0.1233 - acc: 0.9800 - val_loss: 0.2429 - val_acc: 0.9262\n",
      "Epoch 21/25\n",
      "313/313 [==============================] - 27s 88ms/step - loss: 0.1035 - acc: 0.9850 - val_loss: 0.1216 - val_acc: 0.9653\n",
      "Epoch 22/25\n",
      "313/313 [==============================] - 28s 89ms/step - loss: 0.0792 - acc: 0.9901 - val_loss: 0.0798 - val_acc: 0.9903\n",
      "Epoch 23/25\n",
      "313/313 [==============================] - 28s 90ms/step - loss: 0.0645 - acc: 0.9919 - val_loss: 0.0624 - val_acc: 0.9916\n",
      "Epoch 24/25\n",
      "313/313 [==============================] - 28s 90ms/step - loss: 0.1003 - acc: 0.9836 - val_loss: 0.0685 - val_acc: 0.9905\n",
      "Epoch 25/25\n",
      "313/313 [==============================] - 28s 89ms/step - loss: 0.0362 - acc: 0.9986 - val_loss: 0.0507 - val_acc: 0.9890\n"
     ]
    }
   ],
   "source": [
    "model = DateTranslation()\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='nadam', \n",
    "              metrics=['acc'])\n",
    "history = model.fit([x_train, x_train_decoder], y_train, epochs=25, \n",
    "                    validation_data=([x_valid, x_valid_decoder], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "smooth-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_predict_date_strs_v2(date_strs):\n",
    "    x = prepare_date_strs_padded(date_strs)\n",
    "    x_decoder = tf.zeros(shape=(len(x), max_output_length), dtype=tf.int32)\n",
    "    y_probas = model.predict([x, x_decoder])\n",
    "    y_pred = tf.argmax(y_probas, axis=-1)\n",
    "    return ids_to_date_strs(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "descending-virtue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_predict_date_strs_v2((['May 02, 2020', 'July 14, 1789']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-elevation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
